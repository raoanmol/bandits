{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_arms = np.random.randint(2, 11)\n",
    "means = np.random.uniform(0, 1, num_arms)\n",
    "std_devs = np.random.uniform(0.1, 0.5, num_arms)\n",
    "\n",
    "T = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num_arms`: generates a random number fo arms between **2** and **10**.<br>\n",
    "`means`: generates a random mean between **0** and **1** for all arms.<br>\n",
    "`std_devs`: generates a random standard deviation between **0.1** and **0.5** for all arms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_arm(arm_index, means, std_devs):\n",
    "    reward = np.random.normal(means[arm_index], std_devs[arm_index])\n",
    "    reward = np.clip(reward, 0, 1)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pull_arm(arm_index, means, std_devs)` generates a random reward for an arm based on its mean and standard deviation with an upper and lower limit of 1 and 0 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward possible: 572.9568943630386\n",
      "Reward obtaines: 486.71232345990273\n",
      "Regret score: 86.24457090313587\n"
     ]
    }
   ],
   "source": [
    "# Explore only\n",
    "max_reward = np.max(means) * T\n",
    "total_reward = 0\n",
    "\n",
    "for _ in range(T):\n",
    "    rand_arm = np.random.randint(0, num_arms)\n",
    "    reward = pull_arm(rand_arm, means, std_devs)\n",
    "    total_reward += reward\n",
    "\n",
    "regret = max_reward - total_reward\n",
    "\n",
    "print(\"Max reward possible: \" + str(max_reward))\n",
    "print(\"Reward obtaines: \" + str(total_reward))\n",
    "print(\"Regret score: \" + str(regret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore only** selects a completely random arm for T iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward possible: 572.9568943630386\n",
      "Reward obtaines: 534.5179761067956\n",
      "Regret score: 38.43891825624303\n"
     ]
    }
   ],
   "source": [
    "# Exploit only\n",
    "max_reward = np.max(means) * T\n",
    "total_reward = 0\n",
    "\n",
    "initial_rewards = [0] * num_arms\n",
    "for arm in range(num_arms):\n",
    "    reward = pull_arm(arm, means, std_devs)\n",
    "    initial_rewards[arm] = reward\n",
    "    total_reward += reward\n",
    "\n",
    "max_arm = initial_rewards.index(np.max(initial_rewards))\n",
    "\n",
    "for _ in range(T - num_arms):\n",
    "    reward = pull_arm(max_arm, means, std_devs)\n",
    "    total_reward += reward\n",
    "\n",
    "regret = max_reward - total_reward\n",
    "\n",
    "print(\"Max reward possible: \" + str(max_reward))\n",
    "print(\"Reward obtaines: \" + str(total_reward))\n",
    "print(\"Regret score: \" + str(regret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploit only** explores each arm once, uses the highest value observed and keeps exploiting it for all instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward possible: 572.9568943630386\n",
      "Reward obtained: 543.9385006659616\n",
      "Regret score: 29.018393697077045\n"
     ]
    }
   ],
   "source": [
    "# Epsilon-greedy\n",
    "epsilon = 0.1\n",
    "\n",
    "total_reward = 0\n",
    "counts = [0] * num_arms\n",
    "cur_means = [0] * num_arms\n",
    "\n",
    "for arm in range(num_arms):\n",
    "    reward = pull_arm(arm, means, std_devs)\n",
    "    cur_means[arm] = reward\n",
    "    counts[arm] += 1\n",
    "    total_reward += reward\n",
    "\n",
    "for t in range(T - num_arms):\n",
    "    if np.random.rand() < epsilon:\n",
    "        chosen_arm = np.random.randint(0, num_arms)\n",
    "    else:\n",
    "        chosen_arm = np.argmax(cur_means)\n",
    "\n",
    "    reward = pull_arm(chosen_arm, means, std_devs)\n",
    "    total_reward += reward\n",
    "\n",
    "    counts[chosen_arm] += 1\n",
    "    cur_means[chosen_arm] += (reward - cur_means[chosen_arm]) / counts[chosen_arm]\n",
    "\n",
    "regret = max_reward - total_reward\n",
    "\n",
    "print(\"Max reward possible: \" + str(max_reward))\n",
    "print(\"Reward obtained: \" + str(total_reward))\n",
    "print(\"Regret score: \" + str(regret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epsilon-greedy** sets initial means. Then a small *epsilon* value to randomly decide when to explore. Then we continue randomly exploring or exploiting and we keep updating the mean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward possible: 572.9568943630386\n",
      "Reward obtained: 528.0594981343785\n",
      "Regret score: 44.897396228660114\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "counts = [0] * num_arms\n",
    "cur_means = [0] * num_arms\n",
    "\n",
    "for arm in range(num_arms):\n",
    "    reward = pull_arm(arm, means, std_devs)\n",
    "    total_reward += reward\n",
    "    counts[arm] += 1\n",
    "    cur_means[arm] = reward\n",
    "\n",
    "for t in range(num_arms, T):\n",
    "    ucb_vals = [0] * num_arms\n",
    "    for arm in range(num_arms):\n",
    "        if counts[arm] > 0:\n",
    "            ucb_vals[arm] = cur_means[arm] + np.sqrt((2 * np.log(t)) / counts[arm])\n",
    "        else:\n",
    "            ucb_vals[arm] = float('inf')\n",
    "\n",
    "    chosen_arm = np.argmax(ucb_vals)\n",
    "\n",
    "    reward = pull_arm(chosen_arm, means, std_devs)\n",
    "    total_reward += reward\n",
    "\n",
    "    counts[chosen_arm] += 1\n",
    "    cur_means[chosen_arm] += (reward - cur_means[chosen_arm]) / counts[chosen_arm]\n",
    "\n",
    "regret = max_reward - total_reward\n",
    "\n",
    "print(\"Max reward possible: \" + str(max_reward))\n",
    "print(\"Reward obtained: \" + str(total_reward))\n",
    "print(\"Regret score: \" + str(regret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upper Confidence Bound (USB1)** tries to strike a balance between exploring and exploiting. The goal is to maximize a certain value which takes into account how many tmes you've explored an arm. This allows for a balance between exploring arms and exploiting them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bandits_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
